{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import ellip, lfilter,butter,find_peaks\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"SNN\"\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_VALUE = 1337\n",
    "\n",
    "torch.manual_seed(SEED_VALUE)\n",
    "np.random.seed(SEED_VALUE)\n",
    "random.seed(SEED_VALUE)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"using cuda\")\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.manual_seed_all(SEED_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_figures(original_data, events, label, predict_data, single_label):\n",
    "    \n",
    "    ind_ = range(0, 800)\n",
    "    # print(events.shape, label.shape, predict_data.shape)\n",
    "\n",
    "    fig, axs = plt.subplots(4, 1, layout='constrained')\n",
    "    \n",
    "    axs[0].stem(events[0, ind_])\n",
    "    axs[0].set_ylabel(\"Events\")\n",
    "\n",
    "    axs[1].plot(label[0, ind_])\n",
    "    axs[1].set_ylabel(\"Label\")\n",
    "\n",
    "    axs[2].plot(predict_data[0, ind_])\n",
    "    axs[2].set_ylabel(\"Pred\")\n",
    "\n",
    "    axs[3].plot(single_label[0, ind_])\n",
    "    axs[3].set_ylabel(\"Label_Single\")\n",
    "\n",
    "    plt.savefig(\"./spike_label_plots.jpg\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_spiketrain(filepath, filename, multipier):\n",
    "    MAT = loadmat(os.path.join(filepath, filename + \".mat\"))\n",
    "    data = np.array(MAT['data'])[0]\n",
    "    spikeTime = np.array(MAT['spike_times'])[0][0][0]\n",
    "    sampling_interval =np.array(MAT['samplingInterval'][0][0]) * 1e-3\n",
    "    sample_rate = 1/sampling_interval\n",
    "    b, a = butter(4, [300*2/sample_rate, 5000*2/sample_rate], btype='band')\n",
    "    data = lfilter(b, a, data)\n",
    "\n",
    "    ABS_THD = 4*np.median(np.abs(data) /0.6745)\n",
    "    data_up = np.copy(data)\n",
    "    data_up[data_up < ABS_THD] = 0\n",
    "    peaks, _ = find_peaks(data_up)\n",
    "    data_down = np.copy(data)\n",
    "    data_down[data_down > -ABS_THD] = 0\n",
    "    valleys, _ = find_peaks(abs(data_down))\n",
    "    median_peak = np.median(data_up[peaks])\n",
    "    median_valleys = np.median(data_down[valleys])\n",
    "    spike_amplitude = (median_peak - median_valleys) / 2\n",
    "    modulation_thd = spike_amplitude * multipier\n",
    "    \n",
    "    pulseTrain = np.load(os.path.join(filepath, filename + \".npy\"))\n",
    "    # ON_Threshold = modulation_thd\n",
    "    # OFF_Threshold = -modulation_thd\n",
    "    # pulseTrain = delta_modulation_synced(data, ON_Threshold, OFF_Threshold)\n",
    "\n",
    "    spikeTimeGT = np.array(MAT['OVERLAP_DATA'] > 0).astype(np.float32)\n",
    "    data_len = spikeTimeGT.shape[1]\n",
    "    spikeTimeGT = np.insert(spikeTimeGT, 0, [0 for _ in range(22)])\n",
    "    spikeTimeGT = spikeTimeGT[:data_len].reshape(1, -1)\n",
    "    \n",
    "    spikeTimeGT_id = np.array(MAT['spike_times'])[0][0][0] + 22\n",
    "\n",
    "    return data, pulseTrain, spikeTimeGT, sample_rate, spikeTimeGT_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath, filename):\n",
    "    original_data, pulseTrain, spikeTimeGT, sample_rate, spikeTimeGT_single = convert_data_to_spiketrain(filepath, filename, multipier=0.3)\n",
    "    \n",
    "    spike_data_ind = (pulseTrain[0]/10).astype(int).reshape(-1,1)\n",
    "    spike_num = pulseTrain[1].reshape(-1,1)\n",
    "    spikeTimeGT_single = spikeTimeGT_single.astype(int)\n",
    "\n",
    "    spike = np.zeros_like(spikeTimeGT)\n",
    "    for i, id in enumerate(spike_data_ind):\n",
    "        spike[:, id] = spike_num[i]\n",
    "\n",
    "    label_single = np.zeros_like(spikeTimeGT)\n",
    "    for i, id in enumerate(spikeTimeGT_single):\n",
    "        label_single[:, id] = np.ones(1)\n",
    "\n",
    "    draw_figures(original_data=original_data, events=spike, label=spikeTimeGT, predict_data=spikeTimeGT, single_label=label_single)\n",
    "    \n",
    "    return original_data, spike, spikeTimeGT, sample_rate, label_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_3d(samples, labels, overlap=True, stride=1, bin_width=2, num_steps=6, model_type=\"ANN\"):\n",
    "    if overlap:\n",
    "        advance_num = stride\n",
    "        bin_width_num = bin_width\n",
    "    else:\n",
    "        advance_num = stride\n",
    "        bin_width_num = stride\n",
    "    \n",
    "    new_samples, new_labels = [], []\n",
    "    temp_sample = torch.zeros((samples.shape[0], int(samples.shape[1] // advance_num), bin_width_num), dtype=torch.float32)\n",
    "    temp_label = torch.zeros((labels.shape[0], int(samples.shape[1] // advance_num)), dtype=torch.float32)\n",
    "\n",
    "    for col in range(temp_sample.shape[1]):\n",
    "        if col <  bin_width_num/advance_num:\n",
    "            bin_start = 0\n",
    "            bin_end = int(col * advance_num)\n",
    "            if col == 0:\n",
    "                bin_end = 1\n",
    "            temp_sample[:, col, bin_start:bin_end] = samples[:, bin_start: bin_end]\n",
    "            # continue\n",
    "        else:\n",
    "            bin_start = int(col * advance_num - bin_width_num)\n",
    "            bin_end = int(col * advance_num)\n",
    "            temp_sample[:, col, :] = samples[:, bin_start: bin_end]\n",
    "\n",
    "        # temp_label[:, col] = 1 if 1 in labels[:, bin_start: bin_end] else 0\n",
    "        temp_label[:, col] = labels[:, col * advance_num]\n",
    "\n",
    "    if num_steps < bin_width_num:\n",
    "        sum_num = bin_width_num // num_steps\n",
    "        temp_sample_num_steps = torch.zeros((int(bin_width_num//num_steps), temp_sample.shape[1], num_steps), dtype=torch.float32)\n",
    "        for idx in range(num_steps):\n",
    "            start_idx = idx*sum_num\n",
    "            end_idx = idx*sum_num + sum_num\n",
    "            temp_sample_num_steps[:, :, idx] = temp_sample[:, :, start_idx: end_idx].squeeze().t()\n",
    "            # temp_sample_num_steps[:, :, idx] = torch.sum(abs(temp_sample[:, :, start_idx: end_idx]), dim=2)\n",
    "\n",
    "        if model_type == 'ANN':\n",
    "            new_samples.append(temp_sample_num_steps)\n",
    "        else:\n",
    "            new_samples.append(torch.sign(temp_sample_num_steps))\n",
    "    else:\n",
    "\n",
    "        if model_type == 'ANN':\n",
    "            new_samples.append(temp_sample)\n",
    "        else:\n",
    "            new_samples.append(torch.sign(temp_sample))\n",
    "\n",
    "    new_labels.append(temp_label)\n",
    "\n",
    "    return new_samples, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNNModelTraining(nn.Module):\n",
    "    def __init__(self, input_dim, beta=0.5, mem_threshold=0.5, spike_grad2=surrogate.atan(alpha=2),\n",
    "                 layer1=64, layer2=16, output_dim=1, dropout_rate=0.1, num_step=6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_step = num_step\n",
    "        self.input_dim = input_dim\n",
    "        self.beta = beta\n",
    "        self.spike_grad = spike_grad2\n",
    "        self.mem_threshold = mem_threshold\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, layer1)\n",
    "        self.fc2 = nn.Linear(layer1, layer2)\n",
    "        self.fc3 = nn.Linear(layer2, output_dim)\n",
    "\n",
    "        self.lif1 = snn.Leaky(beta=self.beta, spike_grad=self.spike_grad, threshold=0.5, learn_beta=False, \n",
    "                              learn_threshold=False, init_hidden=False, reset_mechanism=\"none\") # 0.8\n",
    "        self.lif2 = snn.Leaky(beta=self.beta, spike_grad=self.spike_grad, threshold=0.5, learn_beta=False,\n",
    "                              learn_threshold=False, init_hidden=False, reset_mechanism=\"none\") # 0.4\n",
    "        self.lif3 = snn.Leaky(beta=self.beta, spike_grad=self.spike_grad, threshold=0.5, learn_beta=False,\n",
    "                              learn_threshold=False, init_hidden=False, reset_mechanism=\"none\")\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.norm_layer = nn.LayerNorm([self.num_step, self.input_dim])\n",
    "        self.reset_mem = False\n",
    "        self.relu = nn.ReLU()\n",
    "        self.Q_bit = 8\n",
    "        self.quant_max = 1\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.reset_mem:\n",
    "            mem1 = self.lif1.init_leaky()\n",
    "            mem2 = self.lif2.init_leaky()\n",
    "            mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        # x = self.norm_layer(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        scale_f=(2**self.Q_bit-1)/self.quant_max\n",
    "            \n",
    "        for step in range(self.num_step):\n",
    "            \n",
    "            input_ = x[:, step, :]\n",
    "            cur1 = self.dropout(self.fc1(input_))\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "\n",
    "            cur2 = self.dropout(self.fc2(spk1))\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "            # mem1 = torch.round(scale_f*torch.clamp(mem1, -self.quant_max, self.quant_max))/scale_f\n",
    "            # mem2 = torch.round(scale_f*torch.clamp(mem2, -self.quant_max, self.quant_max))/scale_f\n",
    "            # mem3 = torch.round(scale_f*torch.clamp(mem3, -self.quant_max, self.quant_max))/scale_f\n",
    "\n",
    "        return spk3   # Training Mode only need to return the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(samples, train_ratio=0.5):\n",
    "    len_dataset = samples.shape[1]\n",
    "    train_len = int(len_dataset*train_ratio)\n",
    "    ind_train = list(range(0, train_len))\n",
    "    end_ind_val = int(train_len+train_len//2)\n",
    "    ind_val = list(range(train_len, end_ind_val))\n",
    "    ind_test = list(range(end_ind_val, len_dataset))\n",
    "\n",
    "    return ind_train, ind_val, ind_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, samples, labels):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[:, idx, :]\n",
    "        # sample = self.samples[:, idx]\n",
    "        label = self.labels[:, idx]\n",
    "\n",
    "        return sample, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.samples.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_SPD_metrics(predicted_spike_times, new_labels_single):\n",
    "    # Initialize counters\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    # Iterate through ground truth spike times\n",
    "    \n",
    "    within_window_true = np.where(new_labels_single[:,0]==1)\n",
    "    \n",
    "    num_of_tp = 0\n",
    "    num_of_FN = 0\n",
    "    num_of_FP = 0\n",
    "    for item in within_window_true[0]:\n",
    "        if 1 in predicted_spike_times[item-12: item+12, 0]:\n",
    "            num_of_tp += 1\n",
    "        else:\n",
    "            num_of_FN += 1\n",
    "\n",
    "    true_positives = num_of_tp\n",
    "    false_negatives = num_of_FN\n",
    "\n",
    "    within_window_pred = np.where(predicted_spike_times[:,0]==1)\n",
    "    for item in within_window_pred[0]:\n",
    "        if 1 not in new_labels_single[item-12: item+12, 0]:\n",
    "            num_of_FP += 1\n",
    "   \n",
    "\n",
    "    # Calculate sensitivity, FDR, and accuracy\n",
    "    sensitivity = true_positives / (true_positives + false_negatives)\n",
    "    fdr = num_of_FP/(true_positives + num_of_FP)\n",
    "    accuracy = true_positives / (true_positives+num_of_FP+false_negatives)\n",
    "\n",
    "    return sensitivity, fdr, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refractory_label(pred, label_single, bin_width, stride):\n",
    "    temp_label = torch.zeros((pred.shape[0], pred.shape[1]), dtype=torch.float32)\n",
    "    for col in range(temp_label.shape[0]):\n",
    "        if col <  bin_width/stride:\n",
    "            bin_start = 0\n",
    "            bin_end = int(col * stride)\n",
    "            if col == 0:\n",
    "                bin_end = 1\n",
    "            label_ = 1 if 1 in label_single[:, bin_start:bin_end] else 0\n",
    "            temp_label[col, :] = label_\n",
    "        else:\n",
    "            bin_start = int(col * stride - stride)\n",
    "            bin_end = int(col * stride)\n",
    "            label_ = 1 if 1 in label_single[:, bin_start:bin_end] else 0\n",
    "\n",
    "            temp_label[col, :] = label_\n",
    "    temp_label = np.insert(np.array(temp_label), 0, [0 for _ in range(2)])\n",
    "    temp_label = temp_label[:pred.shape[0]].reshape(1, -1)\n",
    "\n",
    "    return torch.tensor(temp_label).reshape(-1, 1)\n",
    "\n",
    "def refractory_pred(pred, refractory_interval=3):\n",
    "    spike_updated = []\n",
    "    checkpoint = 0\n",
    "    pred_true_id = np.where(pred[:,0]==1)\n",
    "    for item in pred_true_id[0]:\n",
    "        if item > checkpoint:\n",
    "            spike_updated.append(item)\n",
    "            checkpoint = spike_updated[-1] + refractory_interval\n",
    "    spike_updated = np.array(spike_updated).astype(int)\n",
    "    pred_refract = np.zeros_like(pred)\n",
    "    for i, id in enumerate(spike_updated):\n",
    "        pred_refract[id, 0] = 1\n",
    "\n",
    "    return torch.tensor(pred_refract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    y_pred = y_pred.to('cpu')\n",
    "    y_true = y_true.to('cpu')\n",
    "    # x_correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    correct_num = torch.eq(y_true, y_pred).sum().item()\n",
    "    # correct = (x_correct+y_correct)/2\n",
    "    acc = correct_num / y_pred.numel()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(dataset, net, model_weight_name, stride, bin_width, num_steps, ind_train, ind_val):\n",
    "    # dataset = (spikes, labels, label_single)\n",
    "    samples = torch.tensor(dataset[0][:, ind_train[0]: ind_train[-1]+1])\n",
    "    labels = torch.tensor(dataset[1][:, ind_train[0]: ind_train[-1]+1])\n",
    "    labels_single = torch.tensor(dataset[2][:, ind_train[0]: ind_train[-1]+1])\n",
    "\n",
    "    new_samples, new_labels = transform_to_3d(samples, labels, overlap=True, stride=stride, bin_width=bin_width, num_steps=num_steps, model_type=\"ANN\")\n",
    "\n",
    "    new_samples = new_samples[0]\n",
    "    new_labels = new_labels[0]\n",
    "\n",
    "    training_set = MyDataset(new_samples, new_labels)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "                            dataset=training_set,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            drop_last=False,\n",
    "                            shuffle=False,\n",
    "                        )\n",
    "\n",
    "    \n",
    "    criterion = torch.nn.MSELoss()\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    optimiser = torch.optim.AdamW(net.parameters(), lr=0.008, \n",
    "                                  betas=(0.9, 0.999), weight_decay=0) #0.008\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimiser, T_max=EPOCHS+5)\n",
    "    best_training_acc, best_val_acc = float(\"-inf\"), float(\"-inf\")\n",
    "    net.to(DEVICE)\n",
    "    \n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        net.train()\n",
    "        for i, (sample, label) in enumerate(train_loader):\n",
    "            if MODEL_TYPE == \"SNN\":\n",
    "                net.reset_mem = True\n",
    "            sample = sample.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "\n",
    "            pred = net(sample)\n",
    "            # print(pred.shape, label.shape)\n",
    "            \n",
    "            # pred_acc, ind_acc = torch.max(pred, dim=1)\n",
    "            # loss_val = criterion(pred_acc.reshape(-1, 1), label)\n",
    "            # current_acc = accuracy_fn(label, ind_acc.reshape(-1, 1))\n",
    "            # loss_val = criterion(pred, label.squeeze().long())\n",
    "            loss_val = criterion(pred, label)\n",
    "            \n",
    "            current_acc = accuracy_fn(label, pred)\n",
    "\n",
    "            if current_acc > best_training_acc:\n",
    "                best_training_acc = current_acc\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            # draw_figures(original_data=sample, events=sample.detach().cpu().numpy(), label=label.detach().cpu().numpy(), predict_data=pred.detach().cpu().numpy(), )\n",
    "        print(\"Current Training Accuracy: \", current_acc)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        current_val_acc = validation(dataset, net, stride, bin_width, num_steps, ind_val)\n",
    "        if current_val_acc > best_val_acc:\n",
    "            best_val_acc = current_val_acc\n",
    "            torch.save(net.state_dict(), model_weight_name)\n",
    "        # for name,param in net.named_parameters():\n",
    "        #     print(name, param)\n",
    "        print(f\"{epoch} validation ACC: {current_val_acc}\")\n",
    "\n",
    "def validation(dataset, net, stride, bin_width, num_steps, ind_val):\n",
    "    net.eval()\n",
    "    samples = torch.tensor(dataset[0][:, ind_val[0]: ind_val[-1]+1])\n",
    "    labels = torch.tensor(dataset[1][:, ind_val[0]: ind_val[-1]+1])\n",
    "    labels_single = torch.tensor(dataset[2][:, ind_val[0]: ind_val[-1]+1])\n",
    "\n",
    "    new_samples, new_labels = transform_to_3d(samples, labels, overlap=True, stride=stride, bin_width=bin_width, num_steps=num_steps, model_type=\"ANN\")\n",
    "\n",
    "    new_samples = new_samples[0]\n",
    "    new_labels = new_labels[0]\n",
    "\n",
    "    val_set = MyDataset(new_samples, new_labels)\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "                        dataset=val_set,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        drop_last=False,\n",
    "                        shuffle=False,\n",
    "                    )\n",
    "    \n",
    "    val_acc_final = 0\n",
    "    pred_rec = []\n",
    "    with torch.no_grad():\n",
    "        for i, (sample, label) in enumerate(val_loader):\n",
    "            if MODEL_TYPE == \"SNN\":\n",
    "                net.reset_mem = True\n",
    "            sample = sample.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "\n",
    "            pred = net(sample)\n",
    "            pred_rec.append(pred)\n",
    "            val_acc_final += accuracy_fn(label, pred)\n",
    "            # print(torch.eq(label, pred))\n",
    "            # draw_figures(original_data, torch.sum(sample, dim=2).cpu().numpy(), label.cpu().numpy(), pred.cpu().numpy())\n",
    "    \n",
    "    return val_acc_final/(i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataset, net, model_weight_name, stride, bin_width, num_steps, ind_test):\n",
    "    net.load_state_dict(torch.load(model_weight_name))\n",
    "    net.eval()\n",
    "    samples = torch.tensor(dataset[0][:, ind_test[0]: ind_test[-1]+1])\n",
    "    labels = torch.tensor(dataset[1][:, ind_test[0]: ind_test[-1]+1])\n",
    "    labels_single = torch.tensor(dataset[2][:, ind_test[0]: ind_test[-1]+1])\n",
    "\n",
    "    new_samples, new_labels_single = transform_to_3d(samples, labels_single, overlap=True, stride=stride, bin_width=bin_width, num_steps=num_steps, model_type=\"ANN\")\n",
    "    new_samples, new_labels = transform_to_3d(samples, labels, overlap=True, stride=stride, bin_width=bin_width, num_steps=num_steps, model_type=\"ANN\")\n",
    "\n",
    "    new_samples = new_samples[0]\n",
    "    new_labels = new_labels[0]\n",
    "    new_labels_single = new_labels_single[0]\n",
    "    # print(new_samples.shape, new_labels.shape)\n",
    "\n",
    "    test_set = MyDataset(new_samples, new_labels)\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "                        dataset=test_set,\n",
    "                        batch_size=new_samples.shape[1],\n",
    "                        drop_last=False,\n",
    "                        shuffle=False,\n",
    "                    )\n",
    "    \n",
    "    test_acc_final = 0\n",
    "    pred_rec = []\n",
    "    with torch.no_grad():\n",
    "        for i, (sample, label) in enumerate(test_loader):\n",
    "            if MODEL_TYPE == \"SNN\":\n",
    "                net.reset_mem = True\n",
    "            sample = sample.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "\n",
    "            pred = net(sample)\n",
    "            pred_rec.append(pred)\n",
    "            # print(pred[pred==1])\n",
    "            test_acc_final += accuracy_fn(label, pred)\n",
    "\n",
    "    new_labels = new_labels.t()\n",
    "    new_labels_single = new_labels_single.t()\n",
    "    label_refract = refractory_label(pred, labels_single, bin_width, stride)\n",
    "    pred_refract = refractory_pred(pred.cpu().numpy())\n",
    "    # draw_figures(original_data=sample, events=sample.detach().cpu().numpy(), label=label.detach().cpu().numpy(), predict_data=pred_refract.detach().cpu().numpy(), single_label=label_refract.detach().cpu().numpy())\n",
    "    sensitivity, fdr, accuracy = calculate_SPD_metrics(pred_refract.detach().cpu().numpy(), label_refract.detach().cpu().numpy())\n",
    "    \n",
    "    print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "    print(f\"FDR: {fdr:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Simulator_data/'\n",
    "file_name = 'C_Easy2_noise02' # C_Difficult2_noise005, C_Difficult2_noise01, C_Difficult2_noise015, C_Difficult2_noise02, C_Easy2_noise005, C_Easy2_noise01, C_Easy2_noise015, C_Easy2_noise02\n",
    "# C_Easy1_noise005, C_Easy1_noise01, C_Easy1_noise015, C_Easy1_noise02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight_name =  \"./SNN_weight/\" + file_name + \"_model_state_dict.pth\"\n",
    "original_data, spikes, labels, sampling_freq, label_single = load_data(file_path, file_name)\n",
    "\n",
    "sampling_sample_1ms = (sampling_freq/1000).astype(int) #1ms\n",
    "stride = int(sampling_sample_1ms)\n",
    "bin_width = int(sampling_sample_1ms)\n",
    "num_steps = 1\n",
    "\n",
    "ind_train, ind_val, ind_test = split_dataset(spikes, train_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SNNModelTraining(\n",
       "  (fc1): Linear(in_features=24, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=2, bias=True)\n",
       "  (fc3): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (lif1): Leaky()\n",
       "  (lif2): Leaky()\n",
       "  (lif3): Leaky()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (norm_layer): LayerNorm((1, 24), eps=1e-05, elementwise_affine=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = 16\n",
    "layer2 = 2\n",
    "\n",
    "net = SNNModelTraining(input_dim=int(bin_width//num_steps), num_step=num_steps, layer1=layer1, layer2=layer2)\n",
    "net.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Training Accuracy:  0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/64 [00:00<00:41,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 validation ACC: 0.8554550438596491\n",
      "Current Training Accuracy:  0.868421052631579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/64 [00:01<00:37,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation ACC: 0.8592961896929824\n",
      "Current Training Accuracy:  0.8256578947368421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 3/64 [00:01<00:36,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 validation ACC: 0.816149259868421\n",
      "Current Training Accuracy:  0.17434210526315788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 4/64 [00:02<00:35,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 validation ACC: 0.816149259868421\n",
      "Current Training Accuracy:  0.881578947368421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 5/64 [00:03<00:36,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 validation ACC: 0.8634765625\n",
      "Current Training Accuracy:  0.868421052631579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 6/64 [00:03<00:35,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 validation ACC: 0.8644462719298246\n",
      "Current Training Accuracy:  0.6973684210526315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 7/64 [00:04<00:34,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 validation ACC: 0.8662280701754386\n",
      "Current Training Accuracy:  0.881578947368421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 7/64 [00:04<00:39,  1.44it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File ./SNN_weight/C_Easy2_noise02_model_state_dict.pth cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mspikes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_single\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m         \u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmodel_weight_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_weight_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbin_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbin_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m         \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m         \u001b[49m\u001b[43mind_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mind_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m         \u001b[49m\u001b[43mind_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mind_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 63\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(dataset, net, model_weight_name, stride, bin_width, num_steps, ind_train, ind_val)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_val_acc \u001b[38;5;241m>\u001b[39m best_val_acc:\n\u001b[0;32m     62\u001b[0m     best_val_acc \u001b[38;5;241m=\u001b[39m current_val_acc\n\u001b[1;32m---> 63\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_weight_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# for name,param in net.named_parameters():\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m#     print(name, param)\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m validation ACC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_val_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    850\u001b[0m         _save(\n\u001b[0;32m    851\u001b[0m             obj,\n\u001b[0;32m    852\u001b[0m             opened_zipfile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[0;32m    856\u001b[0m         )\n\u001b[0;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:716\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:687\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: File ./SNN_weight/C_Easy2_noise02_model_state_dict.pth cannot be opened."
     ]
    }
   ],
   "source": [
    "training(dataset=(spikes, labels, label_single), \n",
    "         net=net, \n",
    "         model_weight_name=model_weight_name, \n",
    "         stride=stride, \n",
    "         bin_width=bin_width, \n",
    "         num_steps=num_steps, \n",
    "         ind_train=ind_train, \n",
    "         ind_val=ind_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47529/2786906122.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(model_weight_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.9632\n",
      "FDR: 0.0531\n",
      "Accuracy: 0.9138\n"
     ]
    }
   ],
   "source": [
    "test(dataset=(spikes, labels, label_single), \n",
    "    net=net, \n",
    "    model_weight_name=model_weight_name, \n",
    "    stride=stride, \n",
    "    bin_width=bin_width, \n",
    "    num_steps=num_steps, \n",
    "    ind_test=ind_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
